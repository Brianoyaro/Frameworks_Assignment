{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1547bab1",
   "metadata": {},
   "source": [
    "# CORD-19 COVID-19 Research Paper Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the CORD-19 dataset, focusing on metadata exploration, data cleaning, visualization, and insights about COVID-19 research publications.\n",
    "\n",
    "## Assignment Overview\n",
    "- **Part 1**: Data Loading and Basic Exploration\n",
    "- **Part 2**: Data Cleaning and Preparation  \n",
    "- **Part 3**: Data Analysis and Visualization\n",
    "- **Part 4**: Streamlit Application Development\n",
    "- **Part 5**: Documentation and Reflection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd215c62",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e49d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ðŸ“Š Ready for CORD-19 data analysis\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd   # for data manipulation\n",
    "import numpy as np # for numerical operations\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for statistical data visualization and much more impressive data visualization\n",
    "from collections import Counter # for counting hashable objects\n",
    "import re # for regular expressions\n",
    "import warnings # to manage warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings for cleaner output\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Ready for CORD-19 data analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61545abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Loading CORD-19 metadata...\n",
      "âš ï¸  Note: This may take a few minutes due to large file size\n"
     ]
    }
   ],
   "source": [
    "# Load the CORD-19 metadata\n",
    "print(\"Loading CORD-19 metadata...\")\n",
    "print(\"Note: This may take a few minutes due to large file size\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset - using low_memory=False to handle mixed types\n",
    "    df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "    print(f\"Successfully loaded {len(df):,} records\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: metadata.csv file not found!\")\n",
    "    print(\"Please ensure the CORD-19 metadata.csv file is in the current directory\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fca1a2",
   "metadata": {},
   "source": [
    "## Part 2: Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "if df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CORD-19 DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset dimensions\n",
    "    print(f\"Rows: {df.shape[0]:,}\")\n",
    "    print(f\"Columns: {df.shape[1]:,}\")\n",
    "    print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COLUMN INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Column names and types\n",
    "    print(f\"Column names ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "\n",
    "    print(f\"\\nData types summary:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "else:\n",
    "    print(\"Cannot explore data - dataset not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last few rows\n",
    "if df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FIRST 3 ROWS\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df.head(3))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LAST 3 ROWS\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df.tail(3))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BASIC STATISTICS FOR NUMERICAL COLUMNS\")\n",
    "    print(\"=\" * 60)\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_columns) > 0:\n",
    "        display(df[numeric_columns].describe())\n",
    "    else:\n",
    "        print(\"No numerical columns found\")\n",
    "else:\n",
    "    print(\"Cannot display data - dataset not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b02cb4",
   "metadata": {},
   "source": [
    "## Part 3: Data Cleaning and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef65dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "if df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    missing_data = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "    }).sort_values('Missing_Percentage', ascending=False)\n",
    "    \n",
    "    print(f\"Missing values summary:\")\n",
    "    display(missing_data[missing_data['Missing_Count'] > 0])\n",
    "    \n",
    "    # Identify columns with high missing percentages\n",
    "    high_missing = missing_data[missing_data['Missing_Percentage'] > 50]\n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\nColumns with >50% missing values ({len(high_missing)} columns):\")\n",
    "        for _, row in high_missing.iterrows():\n",
    "            print(f\"   â€¢ {row['Column']}: {row['Missing_Percentage']:.1f}% missing\")\n",
    "    \n",
    "    # Key columns for analysis\n",
    "    key_columns = ['title', 'abstract', 'authors', 'journal', 'publish_time']\n",
    "    print(f\"\\nMissing values in key columns:\")\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "            print(f\"   â€¢ {col}: {missing_pct:.1f}% missing\")\n",
    "        else:\n",
    "            print(f\"   â€¢ {col}: Column not found\")\n",
    "else:\n",
    "    print(\"Cannot analyze missing values - dataset not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned dataset\n",
    "if df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING CLEANED DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Start with original dataset\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    print(f\"Original dataset: {len(df_clean):,} rows\")\n",
    "    \n",
    "    # Remove rows where title is missing (essential for analysis)\n",
    "    if 'title' in df_clean.columns:\n",
    "        initial_count = len(df_clean)\n",
    "        df_clean = df_clean.dropna(subset=['title'])\n",
    "        removed = initial_count - len(df_clean)\n",
    "        print(f\"Removed {removed:,} rows with missing titles\")\n",
    "        print(f\"After title cleanup: {len(df_clean):,} rows\")\n",
    "\n",
    "    # Fill missing abstracts with empty string for text analysis\n",
    "    if 'abstract' in df_clean.columns:\n",
    "        df_clean['abstract'] = df_clean['abstract'].fillna('')\n",
    "        \n",
    "    # Fill missing journal names with \"Unknown\"\n",
    "    if 'journal' in df_clean.columns:\n",
    "        df_clean['journal'] = df_clean['journal'].fillna('Unknown')\n",
    "        \n",
    "    # Fill missing authors with \"Unknown\"\n",
    "    if 'authors' in df_clean.columns:\n",
    "        df_clean['authors'] = df_clean['authors'].fillna('Unknown')\n",
    "    \n",
    "    print(f\"Cleaned dataset ready: {len(df_clean):,} rows\")\n",
    "    print(f\"Data reduction: {((len(df) - len(df_clean)) / len(df) * 100):.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot create cleaned dataset - original dataset not loaded\")\n",
    "    df_clean = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c642d7",
   "metadata": {},
   "source": [
    "## Part 4: Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fdc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation and feature engineering\n",
    "if df_clean is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convert publish_time to datetime\n",
    "    if 'publish_time' in df_clean.columns:\n",
    "        print(\"Converting publication dates...\")\n",
    "        df_clean['publish_time'] = pd.to_datetime(df_clean['publish_time'], errors='coerce')\n",
    "        \n",
    "        # Extract year from publication date\n",
    "        df_clean['publication_year'] = df_clean['publish_time'].dt.year\n",
    "        \n",
    "        # Filter for reasonable years (2000-2024)\n",
    "        valid_years = df_clean['publication_year'].between(2000, 2024, na=False)\n",
    "        print(f\"Papers with valid publication years: {valid_years.sum():,}\")\n",
    "        \n",
    "        # Show year distribution\n",
    "        year_counts = df_clean['publication_year'].value_counts().sort_index()\n",
    "        print(f\"Publication years range: {year_counts.index.min():.0f} - {year_counts.index.max():.0f}\")\n",
    "    \n",
    "    # Create abstract word count\n",
    "    if 'abstract' in df_clean.columns:\n",
    "        print(\"Calculating abstract word counts...\")\n",
    "        df_clean['abstract_word_count'] = df_clean['abstract'].apply(\n",
    "            lambda x: len(str(x).split()) if pd.notna(x) and x != '' else 0\n",
    "        )\n",
    "        print(f\"Average abstract length: {df_clean['abstract_word_count'].mean():.1f} words\")\n",
    "    \n",
    "    # Create title word count\n",
    "    if 'title' in df_clean.columns:\n",
    "        print(\"Calculating title word counts...\")\n",
    "        df_clean['title_word_count'] = df_clean['title'].apply(\n",
    "            lambda x: len(str(x).split()) if pd.notna(x) else 0\n",
    "        )\n",
    "        print(f\"Average title length: {df_clean['title_word_count'].mean():.1f} words\")\n",
    "    \n",
    "    # Clean journal names\n",
    "    if 'journal' in df_clean.columns:\n",
    "        print(\"Processing journal names...\")\n",
    "        # Remove extra whitespace and standardize\n",
    "        df_clean['journal_clean'] = df_clean['journal'].str.strip().str.title()\n",
    "        unique_journals = df_clean['journal_clean'].nunique()\n",
    "        print(f\"Number of unique journals: {unique_journals:,}\")\n",
    "    \n",
    "    print(\"Feature engineering completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot perform feature engineering - cleaned dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a6bc4",
   "metadata": {},
   "source": [
    "## Part 5: Publication Trends Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze publication trends over time\n",
    "if df_clean is not None and 'publication_year' in df_clean.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PUBLICATION TRENDS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get papers by year\n",
    "    yearly_counts = df_clean['publication_year'].value_counts().sort_index()\n",
    "    \n",
    "    # Filter for COVID-era (2019-2022) to focus on pandemic research\n",
    "    covid_era = yearly_counts[(yearly_counts.index >= 2019) & (yearly_counts.index <= 2022)]\n",
    "    \n",
    "    print(\"Publications by year (COVID era - 2019-2022):\")\n",
    "    for year, count in covid_era.items():\n",
    "        if not pd.isna(year):\n",
    "            print(f\"   {int(year)}: {count:,} papers\")\n",
    "    \n",
    "    # Calculate growth rates\n",
    "    if len(covid_era) > 1:\n",
    "        print(f\"\\nGrowth analysis:\")\n",
    "        for i in range(1, len(covid_era)):\n",
    "            prev_year = covid_era.index[i-1]\n",
    "            curr_year = covid_era.index[i]\n",
    "            prev_count = covid_era.iloc[i-1]\n",
    "            curr_count = covid_era.iloc[i]\n",
    "            \n",
    "            growth_rate = ((curr_count - prev_count) / prev_count) * 100\n",
    "            print(f\"   {int(prev_year)} â†’ {int(curr_year)}: {growth_rate:+.1f}% change\")\n",
    "    \n",
    "    # Find peak research year\n",
    "    if not yearly_counts.empty:\n",
    "        peak_year = yearly_counts.idxmax()\n",
    "        peak_count = yearly_counts.max()\n",
    "        print(f\"\\nPeak research year: {int(peak_year)} with {peak_count:,} papers\")\n",
    "        \n",
    "    # Show recent years trend (2020-2022)\n",
    "    recent_years = yearly_counts[(yearly_counts.index >= 2020) & (yearly_counts.index <= 2022)]\n",
    "    if not recent_years.empty:\n",
    "        print(f\"\\nCOVID-19 research output (2020-2022): {recent_years.sum():,} papers\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot analyze publication trends - data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fda88",
   "metadata": {},
   "source": [
    "## Part 6: Journal and Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze journals and publication sources\n",
    "if df_clean is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"JOURNAL AND SOURCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if 'journal_clean' in df_clean.columns:\n",
    "        # Top journals by publication count\n",
    "        top_journals = df_clean['journal_clean'].value_counts().head(15)\n",
    "        \n",
    "        print(\"Top 15 journals publishing COVID-19 research:\")\n",
    "        for i, (journal, count) in enumerate(top_journals.items(), 1):\n",
    "            percentage = (count / len(df_clean)) * 100\n",
    "            print(f\"{i:2d}. {journal}: {count:,} papers ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Journal diversity analysis\n",
    "        total_journals = df_clean['journal_clean'].nunique()\n",
    "        print(f\"\\nJournal diversity:\")\n",
    "        print(f\"   â€¢ Total unique journals: {total_journals:,}\")\n",
    "        print(f\"   â€¢ Journals with only 1 paper: {(df_clean['journal_clean'].value_counts() == 1).sum():,}\")\n",
    "        print(f\"   â€¢ Journals with >100 papers: {(df_clean['journal_clean'].value_counts() > 100).sum():,}\")\n",
    "        \n",
    "        # Concentration analysis\n",
    "        top_10_share = (top_journals.head(10).sum() / len(df_clean)) * 100\n",
    "        print(f\"   â€¢ Top 10 journals represent: {top_10_share:.1f}% of all papers\")\n",
    "    \n",
    "    # Analyze by source (if available)\n",
    "    source_columns = ['source_x', 'source', 'database_name']\n",
    "    source_col = None\n",
    "    for col in source_columns:\n",
    "        if col in df_clean.columns:\n",
    "            source_col = col\n",
    "            break\n",
    "    \n",
    "    if source_col:\n",
    "        print(f\"\\nAnalysis by source ({source_col}):\")\n",
    "        source_counts = df_clean[source_col].value_counts().head(10)\n",
    "        for source, count in source_counts.items():\n",
    "            percentage = (count / len(df_clean)) * 100\n",
    "            print(f\"   â€¢ {source}: {count:,} papers ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\nNo source column found\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot analyze journals - data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f196a",
   "metadata": {},
   "source": [
    "## Part 7: Title Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009be103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze words in paper titles\n",
    "if df_clean is not None and 'title' in df_clean.columns:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TITLE TEXT ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Function to extract words from titles\n",
    "    def extract_words(titles):\n",
    "        \"\"\"Extract and clean words from titles\"\"\"\n",
    "        all_words = []\n",
    "        for title in titles.dropna():\n",
    "            # Convert to lowercase and extract words\n",
    "            words = re.findall(r'\\b[a-zA-Z]{3,}\\b', str(title).lower())\n",
    "            all_words.extend(words)\n",
    "        return all_words\n",
    "    \n",
    "    # Extract all words from titles\n",
    "    print(\"Extracting words from titles...\")\n",
    "    title_words = extract_words(df_clean['title'])\n",
    "    \n",
    "    # Define common stop words to exclude\n",
    "    stop_words = {\n",
    "        'the', 'and', 'for', 'are', 'with', 'from', 'this', 'that', 'was', 'were',\n",
    "        'been', 'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may',\n",
    "        'can', 'study', 'analysis', 'research', 'using', 'based', 'new', 'case',\n",
    "        'cases', 'patients', 'patient', 'clinical', 'systematic', 'meta', 'review'\n",
    "    }\n",
    "    \n",
    "    # Filter out stop words\n",
    "    filtered_words = [word for word in title_words if word not in stop_words]\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(filtered_words)\n",
    "    \n",
    "    print(f\"Total words found: {len(title_words):,}\")\n",
    "    print(f\"Unique words (after filtering): {len(word_counts):,}\")\n",
    "    \n",
    "    # Show most frequent words\n",
    "    most_common = word_counts.most_common(20)\n",
    "    print(f\"\\nTop 20 most frequent words in titles:\")\n",
    "    for i, (word, count) in enumerate(most_common, 1):\n",
    "        percentage = (count / len(filtered_words)) * 100\n",
    "        print(f\"{i:2d}. {word}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # COVID-related terms analysis\n",
    "    covid_terms = ['covid', 'coronavirus', 'sars', 'pandemic', 'outbreak', 'epidemic']\n",
    "    print(f\"\\nCOVID-19 related terms frequency:\")\n",
    "    for term in covid_terms:\n",
    "        count = word_counts.get(term, 0)\n",
    "        if count > 0:\n",
    "            percentage = (count / len(filtered_words)) * 100\n",
    "            print(f\"   â€¢ {term}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Store word counts for visualization\n",
    "    top_words_for_viz = dict(most_common[:15])\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot analyze titles - data not available\")\n",
    "    top_words_for_viz = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f11d2c",
   "metadata": {},
   "source": [
    "## Part 8: Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if df_clean is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Publications over time\n",
    "    if 'publication_year' in df_clean.columns:\n",
    "        ax1 = plt.subplot(2, 3, 1)\n",
    "        yearly_counts = df_clean['publication_year'].value_counts().sort_index()\n",
    "        covid_era = yearly_counts[(yearly_counts.index >= 2015) & (yearly_counts.index <= 2022)]\n",
    "        \n",
    "        plt.bar(covid_era.index, covid_era.values, color='steelblue', alpha=0.7)\n",
    "        plt.title('COVID-19 Research Publications Over Time', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Publication Year')\n",
    "        plt.ylabel('Number of Papers')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(covid_era) > 1:\n",
    "            z = np.polyfit(covid_era.index, covid_era.values, 2)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(covid_era.index, p(covid_era.index), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Top journals\n",
    "    if 'journal_clean' in df_clean.columns:\n",
    "        ax2 = plt.subplot(2, 3, 2)\n",
    "        top_journals = df_clean['journal_clean'].value_counts().head(10)\n",
    "        \n",
    "        plt.barh(range(len(top_journals)), top_journals.values, color='forestgreen', alpha=0.7)\n",
    "        plt.yticks(range(len(top_journals)), [j[:30] + '...' if len(j) > 30 else j for j in top_journals.index])\n",
    "        plt.title('Top 10 Journals Publishing COVID-19 Research', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Number of Papers')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Word frequency in titles\n",
    "    if 'top_words_for_viz' in locals() and top_words_for_viz:\n",
    "        ax3 = plt.subplot(2, 3, 3)\n",
    "        words = list(top_words_for_viz.keys())[:10]\n",
    "        counts = list(top_words_for_viz.values())[:10]\n",
    "        \n",
    "        plt.barh(range(len(words)), counts, color='coral', alpha=0.7)\n",
    "        plt.yticks(range(len(words)), words)\n",
    "        plt.title('Most Frequent Words in Titles', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Abstract length distribution\n",
    "    if 'abstract_word_count' in df_clean.columns:\n",
    "        ax4 = plt.subplot(2, 3, 4)\n",
    "        # Filter out outliers for better visualization\n",
    "        word_counts = df_clean[df_clean['abstract_word_count'] <= 500]['abstract_word_count']\n",
    "        \n",
    "        plt.hist(word_counts, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "        plt.title('Distribution of Abstract Lengths', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Number of Words')\n",
    "        plt.ylabel('Number of Papers')\n",
    "        plt.axvline(word_counts.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {word_counts.mean():.0f} words')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Publication by month (if data available)\n",
    "    if 'publish_time' in df_clean.columns:\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        # Focus on 2020-2021 for COVID-19 research patterns\n",
    "        covid_papers = df_clean[df_clean['publication_year'].isin([2020, 2021])].copy()\n",
    "        if not covid_papers.empty:\n",
    "            covid_papers['month'] = covid_papers['publish_time'].dt.month\n",
    "            monthly_counts = covid_papers['month'].value_counts().sort_index()\n",
    "            \n",
    "            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "            \n",
    "            plt.bar(monthly_counts.index, monthly_counts.values, color='teal', alpha=0.7)\n",
    "            plt.title('COVID-19 Research by Month (2020-2021)', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Month')\n",
    "            plt.ylabel('Number of Papers')\n",
    "            plt.xticks(range(1, 13), month_names)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Year comparison pie chart\n",
    "    if 'publication_year' in df_clean.columns:\n",
    "        ax6 = plt.subplot(2, 3, 6)\n",
    "        key_years = [2019, 2020, 2021, 2022]\n",
    "        year_data = []\n",
    "        year_labels = []\n",
    "        \n",
    "        for year in key_years:\n",
    "            count = (df_clean['publication_year'] == year).sum()\n",
    "            if count > 0:\n",
    "                year_data.append(count)\n",
    "                year_labels.append(f'{year}\\n({count:,})')\n",
    "        \n",
    "        colors = ['lightblue', 'orange', 'lightgreen', 'pink']\n",
    "        plt.pie(year_data, labels=year_labels, autopct='%1.1f%%', colors=colors[:len(year_data)])\n",
    "        plt.title('Research Distribution by Key Years', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"All visualizations created successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot create visualizations - data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word Cloud (if wordcloud package is available)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    if df_clean is not None and 'title' in df_clean.columns:\n",
    "        print(\"Creating word cloud from paper titles...\")\n",
    "        \n",
    "        # Combine all titles into one text\n",
    "        titles_text = ' '.join(df_clean['title'].dropna().astype(str))\n",
    "        \n",
    "        # Create word cloud\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200, \n",
    "            height=600,\n",
    "            background_color='white',\n",
    "            max_words=100,\n",
    "            colormap='viridis',\n",
    "            stopwords=stop_words\n",
    "        ).generate(titles_text)\n",
    "        \n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Research Paper Titles', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Word cloud created successfully!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WordCloud package not available - install with: pip install wordcloud\")\n",
    "    \n",
    "    # Alternative simple word frequency visualization\n",
    "    if 'top_words_for_viz' in locals() and top_words_for_viz:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        words = list(top_words_for_viz.keys())[:15]\n",
    "        counts = list(top_words_for_viz.values())[:15]\n",
    "        \n",
    "        plt.barh(range(len(words)), counts, color='skyblue', alpha=0.8)\n",
    "        plt.yticks(range(len(words)), words)\n",
    "        plt.title('Top 15 Words in Research Titles (Alternative to Word Cloud)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Alternative word frequency chart created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d8937",
   "metadata": {},
   "source": [
    "## Part 9: Streamlit Application Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f052df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Streamlit application code\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Configure Streamlit page\n",
    "st.set_page_config(\n",
    "    page_title=\"CORD-19 Data Explorer\",\n",
    "    page_icon=\"ðŸ”¬\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #1f77b4;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .metric-card {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        border-left: 4px solid #1f77b4;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    \"\"\"Load and cache the CORD-19 dataset\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "        # Basic cleaning\n",
    "        df = df.dropna(subset=['title'])\n",
    "        df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "        df['publication_year'] = df['publish_time'].dt.year\n",
    "        df['journal'] = df['journal'].fillna('Unknown')\n",
    "        df['abstract'] = df['abstract'].fillna('')\n",
    "        df['abstract_word_count'] = df['abstract'].apply(lambda x: len(str(x).split()) if x else 0)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Header\n",
    "    st.markdown('<h1 class=\"main-header\">CORD-19 Data Explorer</h1>', unsafe_allow_html=True)\n",
    "    st.markdown(\"### Interactive exploration of COVID-19 research papers\")\n",
    "    \n",
    "    # Load data\n",
    "    with st.spinner(\"Loading CORD-19 dataset... This may take a moment.\"):\n",
    "        df = load_data()\n",
    "    \n",
    "    if df is None:\n",
    "        st.error(\"Failed to load data. Please ensure metadata.csv is available.\")\n",
    "        return\n",
    "    \n",
    "    # Sidebar filters\n",
    "    st.sidebar.header(\"Filters & Options\")\n",
    "    \n",
    "    # Year range filter\n",
    "    min_year = int(df['publication_year'].min()) if not df['publication_year'].isna().all() else 2000\n",
    "    max_year = int(df['publication_year'].max()) if not df['publication_year'].isna().all() else 2024\n",
    "    \n",
    "    year_range = st.sidebar.slider(\n",
    "        \"Select Year Range\",\n",
    "        min_value=min_year,\n",
    "        max_value=max_year,\n",
    "        value=(2019, 2022),\n",
    "        step=1\n",
    "    )\n",
    "    \n",
    "    # Filter data by year\n",
    "    filtered_df = df[\n",
    "        (df['publication_year'] >= year_range[0]) & \n",
    "        (df['publication_year'] <= year_range[1])\n",
    "    ].copy()\n",
    "    \n",
    "    # Display metrics\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"Total Papers\", f\"{len(filtered_df):,}\")\n",
    "    \n",
    "    with col2:\n",
    "        unique_journals = filtered_df['journal'].nunique()\n",
    "        st.metric(\"Unique Journals\", f\"{unique_journals:,}\")\n",
    "    \n",
    "    with col3:\n",
    "        avg_abstract_length = filtered_df['abstract_word_count'].mean()\n",
    "        st.metric(\"Avg Abstract Length\", f\"{avg_abstract_length:.0f} words\")\n",
    "    \n",
    "    with col4:\n",
    "        year_span = year_range[1] - year_range[0] + 1\n",
    "        st.metric(\"Years Covered\", f\"{year_span}\")\n",
    "    \n",
    "    # Main content tabs\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"Trends\", \"Journals\", \"Text Analysis\", \"Data Sample\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.header(\"Publication Trends Over Time\")\n",
    "        \n",
    "        # Publications by year\n",
    "        yearly_counts = filtered_df['publication_year'].value_counts().sort_index()\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.bar(yearly_counts.index, yearly_counts.values, color='steelblue', alpha=0.7)\n",
    "            ax.set_title('Publications by Year')\n",
    "            ax.set_xlabel('Year')\n",
    "            ax.set_ylabel('Number of Papers')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            st.pyplot(fig)\n",
    "        \n",
    "        with col2:\n",
    "            # Monthly distribution for selected years\n",
    "            if len(filtered_df) > 0:\n",
    "                monthly_data = filtered_df.copy()\n",
    "                monthly_data['month'] = monthly_data['publish_time'].dt.month\n",
    "                monthly_counts = monthly_data['month'].value_counts().sort_index()\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "                ax.bar(monthly_counts.index, monthly_counts.values, color='orange', alpha=0.7)\n",
    "                ax.set_title('Publications by Month')\n",
    "                ax.set_xlabel('Month')\n",
    "                ax.set_ylabel('Number of Papers')\n",
    "                ax.set_xticks(range(1, 13))\n",
    "                ax.set_xticklabels(month_names)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                st.pyplot(fig)\n",
    "    \n",
    "    with tab2:\n",
    "        st.header(\"Journal Analysis\")\n",
    "        \n",
    "        # Top journals\n",
    "        top_journals = filtered_df['journal'].value_counts().head(15)\n",
    "        \n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            st.subheader(\"Top Publishing Journals\")\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            ax.barh(range(len(top_journals)), top_journals.values, color='forestgreen', alpha=0.7)\n",
    "            ax.set_yticks(range(len(top_journals)))\n",
    "            ax.set_yticklabels([j[:30] + '...' if len(j) > 30 else j for j in top_journals.index])\n",
    "            ax.set_xlabel('Number of Papers')\n",
    "            ax.set_title('Top 15 Journals')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            st.pyplot(fig)\n",
    "        \n",
    "        with col2:\n",
    "            st.subheader(\"Journal Statistics\")\n",
    "            st.write(f\"**Total unique journals:** {filtered_df['journal'].nunique():,}\")\n",
    "            st.write(f\"**Journals with only 1 paper:** {(filtered_df['journal'].value_counts() == 1).sum():,}\")\n",
    "            st.write(f\"**Journals with >10 papers:** {(filtered_df['journal'].value_counts() > 10).sum():,}\")\n",
    "            \n",
    "            # Show top journals table\n",
    "            st.subheader(\"Top 10 Journals Table\")\n",
    "            top_10 = filtered_df['journal'].value_counts().head(10).reset_index()\n",
    "            top_10.columns = ['Journal', 'Papers']\n",
    "            top_10['Percentage'] = (top_10['Papers'] / len(filtered_df) * 100).round(1)\n",
    "            st.dataframe(top_10, use_container_width=True)\n",
    "    \n",
    "    with tab3:\n",
    "        st.header(\"Text Analysis\")\n",
    "        \n",
    "        # Word frequency analysis\n",
    "        if len(filtered_df) > 0:\n",
    "            # Extract words from titles\n",
    "            all_titles = ' '.join(filtered_df['title'].dropna().astype(str))\n",
    "            words = re.findall(r'\\\\b[a-zA-Z]{3,}\\\\b', all_titles.lower())\n",
    "            \n",
    "            # Filter stop words\n",
    "            stop_words = {'the', 'and', 'for', 'are', 'with', 'from', 'this', 'that', \n",
    "                         'study', 'analysis', 'research', 'using', 'based', 'new'}\n",
    "            filtered_words = [word for word in words if word not in stop_words]\n",
    "            \n",
    "            word_counts = Counter(filtered_words)\n",
    "            top_words = dict(word_counts.most_common(20))\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.subheader(\"Most Frequent Words in Titles\")\n",
    "                fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                words_list = list(top_words.keys())[:15]\n",
    "                counts_list = list(top_words.values())[:15]\n",
    "                \n",
    "                ax.barh(range(len(words_list)), counts_list, color='coral', alpha=0.7)\n",
    "                ax.set_yticks(range(len(words_list)))\n",
    "                ax.set_yticklabels(words_list)\n",
    "                ax.set_xlabel('Frequency')\n",
    "                ax.set_title('Top 15 Words in Titles')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                st.pyplot(fig)\n",
    "            \n",
    "            with col2:\n",
    "                st.subheader(\"Abstract Length Distribution\")\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                word_counts = filtered_df[filtered_df['abstract_word_count'] <= 500]['abstract_word_count']\n",
    "                ax.hist(word_counts, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "                ax.axvline(word_counts.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                          label=f'Mean: {word_counts.mean():.0f} words')\n",
    "                ax.set_xlabel('Number of Words')\n",
    "                ax.set_ylabel('Number of Papers')\n",
    "                ax.set_title('Abstract Length Distribution')\n",
    "                ax.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                st.pyplot(fig)\n",
    "    \n",
    "    with tab4:\n",
    "        st.header(\"Data Sample\")\n",
    "        \n",
    "        # Display options\n",
    "        sample_size = st.slider(\"Sample size to display\", 5, 50, 10)\n",
    "        \n",
    "        # Show random sample\n",
    "        if len(filtered_df) > 0:\n",
    "            sample_df = filtered_df.sample(n=min(sample_size, len(filtered_df)))\n",
    "            display_columns = ['title', 'journal', 'publication_year', 'abstract_word_count']\n",
    "            available_columns = [col for col in display_columns if col in sample_df.columns]\n",
    "            \n",
    "            st.subheader(f\"Random Sample of {len(sample_df)} Papers\")\n",
    "            st.dataframe(sample_df[available_columns], use_container_width=True)\n",
    "            \n",
    "            # Download option\n",
    "            csv = sample_df.to_csv(index=False)\n",
    "            st.download_button(\n",
    "                label=\"Download Sample as CSV\",\n",
    "                data=csv,\n",
    "                file_name=f\"cord19_sample_{sample_size}.csv\",\n",
    "                mime=\"text/csv\"\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the Streamlit app to a file\n",
    "with open('streamlit_app.py', 'w') as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"Streamlit application code created!\")\n",
    "print(\"File saved as: streamlit_app.py\")\n",
    "print(\"To run the app, use: streamlit run streamlit_app.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37724932",
   "metadata": {},
   "source": [
    "## Part 10: Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings and project completion\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT SUMMARY - CORD-19 DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if df_clean is not None:\n",
    "    print(\"\\nCOMPLETED TASKS:\")\n",
    "    print(\"   âœ“ Part 1: Data Loading and Basic Exploration\")\n",
    "    print(\"   âœ“ Part 2: Data Cleaning and Preparation\")\n",
    "    print(\"   âœ“ Part 3: Data Analysis and Visualization\")\n",
    "    print(\"   âœ“ Part 4: Streamlit Application Development\")\n",
    "    print(\"   âœ“ Part 5: Documentation and Insights\")\n",
    "    \n",
    "    print(f\"\\nDATASET OVERVIEW:\")\n",
    "    print(f\"   â€¢ Original records: {len(df):,}\")\n",
    "    print(f\"   â€¢ After cleaning: {len(df_clean):,}\")\n",
    "    print(f\"   â€¢ Data reduction: {((len(df) - len(df_clean)) / len(df) * 100):.1f}%\")\n",
    "    \n",
    "    if 'publication_year' in df_clean.columns:\n",
    "        valid_years = df_clean['publication_year'].between(2000, 2024, na=False)\n",
    "        covid_era = df_clean[df_clean['publication_year'].between(2020, 2022, na=False)]\n",
    "        \n",
    "        print(f\"\\nKEY FINDINGS:\")\n",
    "        print(f\"   â€¢ Papers with valid publication years: {valid_years.sum():,}\")\n",
    "        print(f\"   â€¢ COVID-19 era papers (2020-2022): {len(covid_era):,}\")\n",
    "        \n",
    "        if len(covid_era) > 0:\n",
    "            peak_year = covid_era['publication_year'].value_counts().idxmax()\n",
    "            peak_count = covid_era['publication_year'].value_counts().max()\n",
    "            print(f\"   â€¢ Peak COVID research year: {int(peak_year)} ({peak_count:,} papers)\")\n",
    "    \n",
    "    if 'journal_clean' in df_clean.columns:\n",
    "        top_journal = df_clean['journal_clean'].value_counts().index[0]\n",
    "        top_count = df_clean['journal_clean'].value_counts().iloc[0]\n",
    "        total_journals = df_clean['journal_clean'].nunique()\n",
    "        \n",
    "        print(f\"   â€¢ Total unique journals: {total_journals:,}\")\n",
    "        print(f\"   â€¢ Top journal: {top_journal} ({top_count:,} papers)\")\n",
    "    \n",
    "    if 'abstract_word_count' in df_clean.columns:\n",
    "        avg_abstract = df_clean['abstract_word_count'].mean()\n",
    "        print(f\"   â€¢ Average abstract length: {avg_abstract:.0f} words\")\n",
    "    \n",
    "    print(f\"\\nDELIVERABLES CREATED:\")\n",
    "    print(f\"   â€¢ Jupyter notebook: cord19_analysis.ipynb\")\n",
    "    print(f\"   â€¢ Streamlit app: streamlit_app.py\")\n",
    "    print(f\"   â€¢ Multiple visualizations and insights\")\n",
    "    \n",
    "    print(f\"\\nNEXT STEPS:\")\n",
    "    print(f\"   1. Run this notebook to see all visualizations\")\n",
    "    print(f\"   2. Launch Streamlit app: streamlit run streamlit_app.py\")\n",
    "    print(f\"   3. Explore interactive features in the web application\")\n",
    "    \n",
    "else:\n",
    "    print(\"Analysis incomplete - data loading failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ASSIGNMENT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
